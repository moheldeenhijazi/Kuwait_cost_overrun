---

authors: "Ruqaya Al-Sabah, Moheldeen Hejazi, Anas Abdin"
date: "1/23/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(factoextra)
library(rpart)
library(rpart.plot)
theme_set(theme_minimal())
options(scipen=999)
dev.off()

cost_data <- read.csv("Desktop/Data Cost Study.csv") %>%
  filter(total.value >= 1)

# simplifying the names of each colum
colnames(cost_data)[1] <- "contractor.class"
colnames(cost_data)[2] <- "owner.class"
colnames(cost_data)[3] <- "contract.type"

# graphing the distribution of the target variable (its not normal, very right scewed)
cost_data %>%
  ggplot() + geom_histogram(aes(total.value))

# transforming the target variable using a log function to get rid of the scewedness 
cost_data <- log(cost_data)

# graphing the distribution of the target variable (its now normal, or as close to normal as possible)
cost_data %>%
  ggplot() + geom_histogram(aes(total.value))


```
Linear Model 
```{r}

# Linear Models
cost.linear.model.fit1 <- lm(total.value ~ contractor.class + owner.class + contract.type, data=cost_data)
cost.linear.model.fit2 <- lm(total.value ~ contractor.class + owner.class, data=cost_data)
cost.linear.model.fit3 <- lm(total.value ~ owner.class, data=cost_data)

# Plots 
layout(matrix(c(1,2,3,4),2,2)) 
plot(cost.linear.model.fit3)


# ANOVA to compare the models
anova(cost.linear.model.fit1, cost.linear.model.fit2, test="Chisq")
# I can still ACCEPT the null hypothesis (the models are the same) and safely say that the first model (cost.linear.model.fit1) is not significantly different (at the alpha = 0.05 level) than the second model (cost.linear.model.fit2)

anova(cost.linear.model.fit1, cost.linear.model.fit3, test="Chisq")
# I can REJECT the null hypothesis (the models are the same) and safely say that the first model (cost.linear.model.fit1) IS significantly different (at the alpha = 0.05 level) than the third model (cost.linear.model.fit3), meaning...

# cost.linear.model.fit1 is the best model for the data because it has the lowest RSS of any model and is significantly different than both of the other models (cost.linear.model.fit2, cost.linear.model.fit3)


# GRAPH THE RESULTS 
ggplot(cost_data, aes(y = total.value, x = owner.class + contractor.class + contract.type)) +
  geom_point(size = 2.75, alpha = 0.4) + geom_smooth(size = 1.5, method="lm") + 
  labs(title = "Linear Regression Model", y = "Total Value", x = "")


```
PCA
```{r}

# PCA
cost.pca <- prcomp(cost_data[,c(1:3)], center = TRUE, scale. = TRUE)
summary(cost.pca)

# Visualizations
fviz_eig(cost.pca)
fviz_pca_var(cost.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)

# Using PC data (making PC dataset)
pc.cost.data <- data.frame(cost.pca[["x"]])
pc.cost.data <- cbind(pc.cost.data, cost_data$total.value) 

colnames(pc.cost.data)[4] <- "total.value"



```
Linear Models (Using PCA Data)
```{r}

# Linear Models
cost.linear.model.fit1.pca <- lm(total.value ~ PC1 + PC2 + PC3, data=pc.cost.data)
cost.linear.model.fit2.pca <- lm(total.value ~ PC1 + PC2, data=pc.cost.data)
cost.linear.model.fit3.pca <- lm(total.value ~ PC1 + PC3, data=pc.cost.data)

# Plots 
layout(matrix(c(1,2,3,4),2,2)) 
plot(cost.linear.model.fit1.pca)
plot(cost.linear.model.fit2.pca)
plot(cost.linear.model.fit3.pca)

# ANOVA to compare the models
anova(cost.linear.model.fit1.pca, cost.linear.model.fit2.pca, test="Chisq")
# I can ACCEPT the null hypothesis (the models are the same) and safely say that the first model (cost.linear.model.fit1) isn't significantly different (at the alpha = 0.05 level) than the second model (cost.linear.model.fit2)


anova(cost.linear.model.fit1.pca, cost.linear.model.fit3.pca, test="Chisq")
# I can REJECT the null hypothesis (the models are the same) and safely say that the first model (cost.linear.model.fit1) IS significantly different (at the alpha = 0.05 level) than the third model (cost.linear.model.fit3)


# cost.linear.model.fit1.pca is the best model for the data because it has the lowest RSS of any model and is significantly different than both of the other models (cost.linear.model.fit2.pca, cost.linear.model.fit3.pca)

# GRAPH THE RESULTS 
ggplot(pc.cost.data,aes(y = total.value, x = PC1 + PC2 + PC3)) +
  geom_point(size = 2.75, alpha = 0.7) + 
  geom_smooth(size = 1.5, method="lm", formula = y ~ x) + 
  labs(title = "Linear Model", subtitle = "(using PCA data)", y = "Total Value", x = "")



```
Polynomial Regression
```{r}

# Polynomial Regression Models (2nd and 3rd degree)
poly.regression.fit1 <- cost_data$total.value ~ poly(cost_data$contractor.class + cost_data$contract.type + cost_data$owner.class, 2, raw = TRUE)

poly.regression.fit2 <- cost_data$total.value ~ poly(cost_data$contractor.class + cost_data$contract.type + cost_data$owner.class, 3, raw = TRUE)

anova(poly.regression.fit1, poly.regression.fit2, test="Chisq")


# GRAPH THE RESULTS 
p + geom_smooth(method = "lm", formula = y ~ poly(x, 2)) + labs(title = "Polynomial Regression Model", subtitle = "(2nd degree polynomial)", y = "Total Value", x = "")

p + geom_smooth(method = "lm", formula = y ~ poly(x, 3)) + labs(title = "Polynomial Regression Model", subtitle = "(3rd degree polynomial)", y = "Total Value", x = "")

p + geom_smooth(method = "lm", formula = y ~ poly(x, 4)) + labs(title = "Polynomial Regression Model", subtitle = "(4th degree polynomial)", y = "Total Value", x = "")

p + geom_smooth(method = "lm", formula = y ~ poly(x, 5)) + labs(title = "Polynomial Regression Model", subtitle = "(5th degree polynomial)", y = "Total Value", x = "")

p + geom_smooth(method = "lm", formula = y ~ poly(x, 6)) + labs(title = "Polynomial Regression Model", subtitle = "(6th degree polynomial)", y = "Total Value", x = "")


```
Locally Weighted Regression 
```{r}

# GRAPH 
ggplot(cost_data, aes(y = total.value, x = contractor.class + owner.class + contract.type)) +
  geom_point(size = 2.75, alpha = 0.7) + geom_smooth(size = 1.5, method="loess") + 
  labs(title = "Locally Weighted Regression Model", y = "Total Value", x = "")



```
Regression Trees
```{r}

tree.cost.linear.fit1 <- rpart(total.value ~ contractor.class + owner.class + contract.type, data = cost_data)

rpart.plot(tree.cost.linear.fit1, type = 4)

# Owner class is roughly two times more important to the dependednt variable than contractor class
tree.cost.linear.fit1$variable.importance

# Train
train <- sample(1:nrow(cost_data), nrow(cost_data)/2)

tree.performance.fit1 <- rpart(total.value ~ contractor.class + owner.class + contract.type, subset = train, data = cost_data)

summary(tree.performance.fit1)

tree.performance.fit1$variable.importance

# According to the model, contractor class was the MOST important variable behind owner class and contract type

# GRAPH THE RESULTS
rpart.plot(tree.performance.fit1)
```


